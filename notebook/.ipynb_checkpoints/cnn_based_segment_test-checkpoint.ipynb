{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T02:08:37.794488Z",
     "start_time": "2019-02-04T02:08:31.876082Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T02:19:29.578735Z",
     "start_time": "2019-02-04T02:19:29.342365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“  人们  常  说  生活  是  一  部  教科书  ，  而  血  与  火  的  战争  更  是  不可多得  的  教科书  ，  她  确实  是  名副其实  的  ‘  我  的  大学  ’  。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#处理输入数据\n",
    "s = []\n",
    "with open('../dataset/msr_training.txt','r') as f:\n",
    "    for line in f.readlines():\n",
    "        s.append(line)\n",
    "print(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T02:20:08.168713Z",
     "start_time": "2019-02-04T02:20:07.406737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人们 常 说 生活 是 一 部 教科书 而 血 与 火 的 战争 更 是 不可多得 的 教科书 她 确实 是 名副其实 的 我 的 大学\n"
     ]
    }
   ],
   "source": [
    "#清洗语料，去除标点符号，空格等\n",
    "pattern = re.compile('[“|”|‘|’|\\n|。|，|？|！|、]')\n",
    "def clean(s): #整理一下数据，有些不规范的地方\n",
    "    s = re.sub(pattern,'',s)\n",
    "    s = ' '.join(s.split())\n",
    "    return s\n",
    "s = list(map(clean, s))\n",
    "print(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T02:26:48.206088Z",
     "start_time": "2019-02-04T02:26:47.121942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bessbesssbmesssssbessbmmesbmesbesbmmesssbe\n"
     ]
    }
   ],
   "source": [
    "#生成标签,标签主要分 b:beginning,m:middle,e:end,s:single，后面还有补充一个x，代表没有意义的结尾\n",
    "def generate_label(s):\n",
    "    tmp = s.split()\n",
    "    tmp_label = []\n",
    "    for w in tmp:\n",
    "        if len(w)==1:\n",
    "            tmp_label.append('s')\n",
    "        elif len(w)==2:\n",
    "            tmp_label.append('b')\n",
    "            tmp_label.append('e')\n",
    "        elif len(w)>2:\n",
    "            tmp_label.append('b')\n",
    "            for i in range(len(w)-2):\n",
    "                tmp_label.append('m')\n",
    "            tmp_label.append('e')\n",
    "    return ''.join(tmp_label)\n",
    "label = list(map(generate_label,s))\n",
    "print(label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T02:28:54.868605Z",
     "start_time": "2019-02-04T02:28:53.962030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人们常说生活是一部教科书而血与火的战争更是不可多得的教科书她确实是名副其实的我的大学\n",
      "共有字符 5157\n"
     ]
    }
   ],
   "source": [
    "#这里对s的处理，是去除空格的影响，因为在分词中空格没有具体的意义\n",
    "text = [''.join(v.split()) for v in s]\n",
    "print(text[0])\n",
    "#char_dict是为了存储每个字符而已\n",
    "char_dict = Counter(''.join(text))\n",
    "print('共有字符',len(char_dict))\n",
    "max_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T02:34:18.789750Z",
     "start_time": "2019-02-04T02:34:18.780774Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#建立词到索引的映射和索引到词的映射，一般可以用keras的prepeocessing处理，但自己处理也不难\n",
    "w2idx = {v:i+1 for i,v in enumerate(char_dict.keys())}\n",
    "idx2word = {w2idx[v]:v for v in w2idx.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T02:36:00.017730Z",
     "start_time": "2019-02-04T02:35:58.866767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 7, 21, 22, 23, 24, 17, 10, 11, 12, 25, 26, 27, 7, 28, 29, 30, 27, 17, 31, 17, 32, 33]\n"
     ]
    }
   ],
   "source": [
    "#将训练样本转化为索引的形式\n",
    "train_text = []\n",
    "for v in text:\n",
    "    tmp = []\n",
    "    for c in v:\n",
    "        tmp.append(w2idx[c])\n",
    "    train_text.append(tmp)\n",
    "print(train_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T02:40:49.682568Z",
     "start_time": "2019-02-04T02:40:48.903651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 7, 21, 22, 23, 24, 17, 10, 11, 12, 25, 26, 27, 7, 28, 29, 30, 27, 17, 31, 17, 32, 33, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['b', 'e', 's', 's', 'b', 'e', 's', 's', 's', 'b', 'm', 'e', 's', 's', 's', 's', 's', 'b', 'e', 's', 's', 'b', 'm', 'm', 'e', 's', 'b', 'm', 'e', 's', 'b', 'e', 's', 'b', 'm', 'm', 'e', 's', 's', 's', 'b', 'e', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x']\n"
     ]
    }
   ],
   "source": [
    "#这里输入的长度规定为50，若长度不够50则补充，超过50截取前50，对训练样本和label都要做同样的操作，因为输入和输出是1:1的关系\n",
    "maxlen=50\n",
    "for i,t in enumerate(train_text):\n",
    "    buchong = np.zeros(50,dtype=int).tolist()\n",
    "    train_text[i] = train_text[i] + buchong\n",
    "    train_text[i] = train_text[i][:50]\n",
    "print(train_text[0])\n",
    "for i,t in enumerate(label):\n",
    "    buchong = 'x'* 50\n",
    "    label[i] = list(label[i]) + list(buchong)\n",
    "    label[i] = label[i][:50]\n",
    "print(label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T02:41:59.658207Z",
     "start_time": "2019-02-04T02:41:59.234834Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将label也转化成索引的形式，然后将每一个索引转化为one-hot的形式\n",
    "label_dict = {'b':0,'m':1,'e':2,'s':3,'x':4}\n",
    "label_dict_num = {label_dict[v]:v for v in label_dict.keys()}\n",
    "label_num = [[label_dict[v] for v in k] for k in label]\n",
    "label_sparse = np.zeros((len(train_text),50,5))\n",
    "for i,l in enumerate(label_num):\n",
    "    for k in range(50):\n",
    "        tmp = np.zeros(5)\n",
    "        tmp[label_num[i][k]] = 1\n",
    "        label_sparse[i][k] = tmp\n",
    "print(label_sparse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T02:46:05.286449Z",
     "start_time": "2019-02-04T02:46:04.704753Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#划分训练集和测试集，这里没有用到真正的测试集，知识自己划分了少部分数据来看看效果\n",
    "train_text = np.array(train_text)\n",
    "label_sparse = np.array(label_sparse)\n",
    "X_train,X_test,y_train,y_test = train_test_split(train_text,label_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T02:46:41.554766Z",
     "start_time": "2019-02-04T02:46:28.205045Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "D:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ti...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#这里模型没有做更改，还是和苏神的一样\n",
    "from keras.layers import Dense, Embedding, LSTM, TimeDistributed, Input, Bidirectional\n",
    "from keras.models import Model\n",
    "sequence = Input(shape=(maxlen,), dtype='int32')\n",
    "embedded = Embedding(len(char_dict)+1, 128, input_length=maxlen, mask_zero=True)(sequence)\n",
    "blstm = Bidirectional(LSTM(64, return_sequences=True), merge_mode='sum')(embedded)\n",
    "output = TimeDistributed(Dense(5, activation='softmax'))(blstm)\n",
    "model = Model(input=sequence, output=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T02:46:41.634553Z",
     "start_time": "2019-02-04T02:46:41.620598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 50, 128)           660224    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 64)            98816     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 5)             325       \n",
      "=================================================================\n",
      "Total params: 759,365\n",
      "Trainable params: 759,365\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T02:51:49.162919Z",
     "start_time": "2019-02-04T02:46:47.941452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65193 samples, validate on 21731 samples\n",
      "Epoch 1/2\n",
      "65193/65193 [==============================] - 152s 2ms/step - loss: 0.6616 - acc: 0.7430 - val_loss: 0.4736 - val_acc: 0.8264\n",
      "Epoch 2/2\n",
      "65193/65193 [==============================] - 147s 2ms/step - loss: 0.4367 - acc: 0.8409 - val_loss: 0.4031 - val_acc: 0.8553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x196b3119358>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,\n",
    "          batch_size=128,\n",
    "          validation_data=(X_test,y_test),epochs=2,verbose=1,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T02:54:43.980941Z",
     "start_time": "2019-02-04T02:54:25.446502Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_label = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T03:00:16.245898Z",
     "start_time": "2019-02-04T03:00:16.211988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b': 0.96966994, 'm': 0.006155009, 'e': 0.0010187782, 's': 0.023033256, 'x': 0.00012297866}\n"
     ]
    }
   ],
   "source": [
    "#这里拿测试集的第一条来做样例\n",
    "segment_sample = test_label[0]\n",
    "#这里是为了以后的维特比解码函数用的，将每一个时间步的概率都转化成对应标签的概率\n",
    "f_dict = []\n",
    "for i,v in enumerate(segment_sample):\n",
    "    tmp={}\n",
    "    for k,s in enumerate(['b','m','e','s','x']):\n",
    "        tmp[s] = segment_sample[i][k]\n",
    "    f_dict.append(tmp)\n",
    "print(f_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T03:00:40.701643Z",
     "start_time": "2019-02-04T03:00:40.694663Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#转移概率，单纯用了等概率\n",
    "transfer_matrix = {'be':0.5, \n",
    "      'bm':0.5, \n",
    "      'eb':0.5, \n",
    "      'es':0.5, \n",
    "      'me':0.5, \n",
    "      'mm':0.5,\n",
    "      'sb':0.5, \n",
    "      'ss':0.5\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T03:02:06.421019Z",
     "start_time": "2019-02-04T03:02:06.392096Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi(nodes):\n",
    "    paths = {'b':nodes[0]['b'], 's':nodes[0]['s']} #定义一开始的状态，因为不是beginning就是single,所以只考虑着两种情况\n",
    "    for l in range(1,len(nodes)): #node 是字的长度,l是从第二个状态开始的\n",
    "        paths_ = paths.copy()\n",
    "        paths = {}\n",
    "        for i in nodes[l].keys(): # 对于当前时刻的每一个状态的字典\n",
    "            nows = {}\n",
    "            for j in paths_.keys(): #path_是前一个状态的概率\n",
    "                if j[-1]+i in transfer_matrix.keys():#对于在转移矩阵里的情况，而对于没有出现在转移矩阵里的情况就不给予考虑了\n",
    "                    # j+i 就是当前的状态，path_s[j]：上一个时刻的j状态的概率；\n",
    "                    #                     nodes[l][i]:当前状态的\n",
    "                    nows[j+i]= paths_[j]+nodes[l][i]+transfer_matrix[j[-1]+i]\n",
    "            if nows != {}:\n",
    "                paths[max(nows,key=nows.get)] = nows[max(nows,key=nows.get)]\n",
    "    return max(paths,key=paths.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T03:03:43.302473Z",
     "start_time": "2019-02-04T03:03:43.294494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迄今为止这次灾害已造成１１９人死亡约４００人失踪１０００多人无家可归\n"
     ]
    }
   ],
   "source": [
    "#将测试集的第一条转换回去文字\n",
    "char_sample = [idx2word[v] for v in X_test[0] if v !=0]\n",
    "print(''.join(char_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T03:04:08.298364Z",
     "start_time": "2019-02-04T03:04:08.289388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bebebebesbebmmebesbmmebebmmmmebebebebebebebebebebe\n"
     ]
    }
   ],
   "source": [
    "segment_sample = viterbi(f_dict)\n",
    "print(segment_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T03:16:21.583151Z",
     "start_time": "2019-02-04T03:16:21.572182Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('迄', 'b'),\n",
       " ('今', 'e'),\n",
       " ('为', 'b'),\n",
       " ('止', 'e'),\n",
       " ('这', 'b'),\n",
       " ('次', 'e'),\n",
       " ('灾', 'b'),\n",
       " ('害', 'e'),\n",
       " ('已', 's'),\n",
       " ('造', 'b'),\n",
       " ('成', 'e'),\n",
       " ('１', 'b'),\n",
       " ('１', 'm'),\n",
       " ('９', 'm'),\n",
       " ('人', 'e'),\n",
       " ('死', 'b'),\n",
       " ('亡', 'e'),\n",
       " ('约', 's'),\n",
       " ('４', 'b'),\n",
       " ('０', 'm'),\n",
       " ('０', 'm'),\n",
       " ('人', 'e'),\n",
       " ('失', 'b'),\n",
       " ('踪', 'e'),\n",
       " ('１', 'b'),\n",
       " ('０', 'm'),\n",
       " ('０', 'm'),\n",
       " ('０', 'm'),\n",
       " ('多', 'm'),\n",
       " ('人', 'e'),\n",
       " ('无', 'b'),\n",
       " ('家', 'e'),\n",
       " ('可', 'b'),\n",
       " ('归', 'e')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#先大致看一下效果，接下来再进行具体的分词\n",
    "list(zip(char_sample,segment_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut_test(char,segment):\n",
    "    tmp_res = []\n",
    "    for i in range(len(segment)):\n",
    "        tmp_char = []\n",
    "        if segment[i]==e:\n",
    "            tmp_res.append()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
